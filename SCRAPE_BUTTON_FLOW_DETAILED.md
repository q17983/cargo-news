# ğŸ”„ Complete Flow: What Happens When You Click "Scrape" Button

## ğŸ“ Step-by-Step Execution Flow

### **Step 1: User Clicks "Scrape" Button in Browser**

**Location:** `frontend/components/SourceList.tsx` (line 116-121)

```typescript
<button
  onClick={() => onScrape(source.id)}  // â† This is called
  className="px-3 py-1.5 bg-green-600..."
>
  Scrape
</button>
```

**What happens:**
- User clicks button
- React calls `onScrape(source.id)` function
- `source.id` = UUID like `"faea0d61-79a1-4fd4-847e-6ccf707f98d6"`

---

### **Step 2: Frontend Handler Function**

**Location:** `frontend/app/sources/page.tsx` (line 89-108)

```typescript
const handleScrapeSource = async (sourceId: string) => {
  // Shows confirmation dialog
  if (!confirm(`Start scraping ${sourceName}? ...`)) {
    return;
  }
  
  setScrapingSourceId(sourceId);  // Shows loading state
  try {
    const result = await triggerScrape(sourceId);  // â† API call
    alert(result.message || `Scraping started...`);
  } catch (err) {
    alert('Failed to start scraping: ' + err.message);
  } finally {
    setScrapingSourceId(null);
  }
};
```

**What happens:**
- Shows confirmation dialog
- Calls `triggerScrape(sourceId)` from API client
- Shows alert with result

---

### **Step 3: Frontend API Client**

**Location:** `frontend/lib/api.ts` (line 72-77)

```typescript
export async function triggerScrape(sourceId?: string) {
  const endpoint = sourceId 
    ? `/api/scrape/${sourceId}`  // â† Single source
    : '/api/scrape/all';          // â† All sources
  
  return request<any>(endpoint, {
    method: 'POST',  // â† HTTP POST request
  });
}
```

**What happens:**
- Builds API endpoint: `POST http://localhost:8000/api/scrape/{source_id}`
- Sends HTTP POST request to backend
- Waits for response

**Network Request:**
```
POST http://localhost:8000/api/scrape/faea0d61-79a1-4fd4-847e-6ccf707f98d6
Content-Type: application/json
```

---

### **Step 4: Backend API Route Handler**

**Location:** `app/api/routes/scrape.py` (line 172-188)

```python
@router.post("/{source_id}")
async def trigger_scrape(source_id: UUID, background_tasks: BackgroundTasks):
    """Manually trigger scraping for a specific source."""
    
    # Step 4a: Get source from database
    source = db.get_source(source_id)
    if not source:
        raise HTTPException(status_code=404, detail="Source not found")
    
    # Step 4b: Add scraping task to background queue
    background_tasks.add_task(scrape_source, source_id)  # â† Queues the task
    
    # Step 4c: Return immediately (doesn't wait for scraping to finish)
    return {
        "message": f"Scraping started for source: {source.name}",
        "source_id": str(source_id)
    }
```

**What happens:**
- Receives POST request
- Gets source from database (`app/database/supabase_client.py`)
- **Queues** `scrape_source()` function to run in background
- **Returns immediately** with "Scraping started" message
- **Does NOT wait** for scraping to complete

**Key Point:** The function returns in ~0.1 seconds, but scraping continues in background!

---

### **Step 5: Background Task Execution**

**Location:** `app/api/routes/scrape.py` (line 162-169)

```python
async def scrape_source(source_id: UUID):
    """
    Async wrapper that runs blocking operations in thread pool.
    """
    loop = asyncio.get_event_loop()
    # Runs _scrape_source_sync in a separate thread
    await loop.run_in_executor(None, _scrape_source_sync, source_id)
```

**What happens:**
- FastAPI's `BackgroundTasks` calls this function
- It runs `_scrape_source_sync()` in a **separate thread**
- This prevents blocking the web server

---

### **Step 6: Actual Scraping Function**

**Location:** `app/api/routes/scrape.py` (line 17-159)

```python
def _scrape_source_sync(source_id: UUID):
    """
    This is where the REAL scraping happens!
    Runs in background thread.
    """
    
    # Step 6a: Get source from database
    source = db.get_source(source_id)
    
    # Step 6b: Create scraper (factory selects the right one)
    scraper = ScraperFactory.create_scraper(source.url)
    # For aircargoweek.com â†’ Creates AircargoweekScraper
    # For aircargonews.net â†’ Creates AircargonewsScraper
    
    # Step 6c: Get listing URL
    listing_url = ScraperFactory.get_listing_url(source.url)
    
    # Step 6d: Extract article URLs
    article_urls = scraper.get_article_urls(listing_url, max_pages=3, ...)
    # This calls:
    # - AircargoweekScraper.get_article_urls() â†’ Uses Playwright
    # - OR AircargonewsScraper.get_article_urls() â†’ Uses requests
    
    # Step 6e: Process each article
    for article_url in article_urls:
        # Scrape article content
        article_data = scraper.scrape_article(article_url)
        
        # Generate AI summary
        summary_data = summarizer.summarize(...)
        
        # Save to database
        db.create_article(article)
    
    # Step 6f: Create scraping log
    db.create_scraping_log(...)
```

**What happens:**
- This is the **same code** that standalone scripts use
- But it's called through the API, not directly
- Runs in background thread (you don't see output)
- Takes 5-30 minutes depending on articles

---

## ğŸ” Key Files Involved

### **Frontend (Browser):**
1. `frontend/components/SourceList.tsx` - Button component
2. `frontend/app/sources/page.tsx` - Handler function
3. `frontend/lib/api.ts` - API client (HTTP request)

### **Backend (Server):**
4. `app/main.py` - FastAPI app (receives HTTP request)
5. `app/api/routes/scrape.py` - API route handler
6. `app/api/routes/scrape.py` - `_scrape_source_sync()` function
7. `app/scraper/scraper_factory.py` - Selects scraper
8. `app/scraper/aircargoweek_scraper.py` - OR `aircargonews_scraper.py`
9. `app/ai/summarizer.py` - AI summarization
10. `app/database/supabase_client.py` - Database operations

---

## ğŸ†š Comparison: Direct Script vs Web Button

### **When You Run Script Directly:**
```bash
python3 scrape_aircargoweek.py
```

**Flow:**
```
Terminal â†’ scrape_aircargoweek.py
         â†’ get_or_create_source()
         â†’ scrape_aircargoweek()  # Same as _scrape_source_sync
         â†’ ScraperFactory.create_scraper()
         â†’ AircargoweekScraper.get_article_urls()
         â†’ Scrape articles...
         â†’ Print progress to terminal âœ…
```

**You see:** Real-time output in terminal

---

### **When You Click Web Button:**
```
Browser â†’ Frontend API call
       â†’ Backend API route
       â†’ Background task queue
       â†’ _scrape_source_sync()  # Same function!
       â†’ ScraperFactory.create_scraper()
       â†’ AircargoweekScraper.get_article_urls()
       â†’ Scrape articles...
       â†’ (No output visible) âŒ
```

**You see:** Nothing (runs silently in background)

---

## ğŸ  Local vs Railway Deployment

### **Local (Your Computer):**

**Backend Server:**
```bash
uvicorn app.main:app --reload --port 8000
```
- Runs on: `http://localhost:8000`
- Background tasks run in same process
- Can see logs in terminal
- Can restart to stop tasks

**Frontend:**
```bash
cd frontend && npm run dev
```
- Runs on: `http://localhost:3000`
- Connects to `http://localhost:8000`

**Flow:**
```
Browser (localhost:3000)
    â†“ HTTP POST
FastAPI (localhost:8000)
    â†“ Background task
Scraping runs in same process
    â†“
Supabase (cloud database)
```

---

### **Railway (Online Deployment):**

**Backend Server:**
- Railway runs: `uvicorn app.main:app --host 0.0.0.0 --port $PORT`
- Runs on: `https://your-app.railway.app`
- Background tasks run in same process
- Logs visible in Railway dashboard
- Can restart service to stop tasks

**Frontend:**
- Railway runs: `npm run build && npm start`
- Runs on: `https://your-frontend.railway.app`
- Connects to backend URL

**Flow:**
```
Browser (your-frontend.railway.app)
    â†“ HTTP POST
FastAPI (your-backend.railway.app)
    â†“ Background task
Scraping runs in Railway server
    â†“
Supabase (cloud database)
```

**Differences:**
- âœ… Same code, same flow
- âœ… Works the same way
- âœ… Background tasks work identically
- âš ï¸ Can't see terminal output (check Railway logs)
- âš ï¸ Can't easily stop (restart Railway service)

---

## ğŸ“Š Visual Flow Diagram

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: User clicks "Scrape" button                        â”‚
â”‚ File: frontend/components/SourceList.tsx                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: Frontend handler                                    â”‚
â”‚ File: frontend/app/sources/page.tsx                         â”‚
â”‚ Function: handleScrapeSource(sourceId)                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: API client makes HTTP request                       â”‚
â”‚ File: frontend/lib/api.ts                                   â”‚
â”‚ Function: triggerScrape(sourceId)                           â”‚
â”‚ Request: POST http://localhost:8000/api/scrape/{source_id}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: Backend receives request                            â”‚
â”‚ File: app/api/routes/scrape.py                              â”‚
â”‚ Function: trigger_scrape(source_id, background_tasks)        â”‚
â”‚ Action: background_tasks.add_task(scrape_source, source_id)â”‚
â”‚ Returns: {"message": "Scraping started..."}                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â”‚ (Returns immediately to browser)
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Background task starts                              â”‚
â”‚ File: app/api/routes/scrape.py                             â”‚
â”‚ Function: scrape_source(source_id)                          â”‚
â”‚ Action: Runs _scrape_source_sync() in thread pool           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Actual scraping happens                            â”‚
â”‚ File: app/api/routes/scrape.py                              â”‚
â”‚ Function: _scrape_source_sync(source_id)                    â”‚
â”‚                                                              â”‚
â”‚ 6a. Get source from database                                â”‚
â”‚ 6b. ScraperFactory.create_scraper()                         â”‚
â”‚     â†’ Creates AircargoweekScraper or AircargonewsScraper    â”‚
â”‚ 6c. scraper.get_article_urls()                              â”‚
â”‚     â†’ File: app/scraper/aircargoweek_scraper.py            â”‚
â”‚     â†’ Uses Playwright to extract URLs                       â”‚
â”‚ 6d. For each URL:                                           â”‚
â”‚     - scraper.scrape_article()                              â”‚
â”‚     - summarizer.summarize()                                â”‚
â”‚       â†’ File: app/ai/summarizer.py                          â”‚
â”‚       â†’ Calls Google Gemini API                             â”‚
â”‚     - db.create_article()                                   â”‚
â”‚       â†’ File: app/database/supabase_client.py               â”‚
â”‚       â†’ Saves to Supabase                                   â”‚
â”‚ 6e. db.create_scraping_log()                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”‘ Key Differences

### **Standalone Script (`scrape_aircargoweek.py`):**
- âœ… Runs directly in terminal
- âœ… Shows real-time progress
- âœ… Can stop with Ctrl+C
- âœ… No server needed
- âŒ Must run manually

### **Web Button:**
- âœ… Runs through API
- âœ… Can trigger from anywhere
- âœ… Runs in background (non-blocking)
- âœ… Same scraping code
- âŒ No visible progress (check logs/status)
- âŒ Can't easily stop (restart server)

---

## ğŸ’¡ Why You Don't See Progress

**The scraping happens in a background thread:**
- Browser gets response immediately (~0.1 seconds)
- Scraping continues in background (5-30 minutes)
- No connection between browser and background task
- Progress is logged to server logs, not sent to browser

**To see progress:**
1. **Check server logs** (terminal running uvicorn)
2. **Check Supabase** (`scraping_logs` table)
3. **Check Sources page** (shows last status, auto-refreshes)
4. **Use API:** `GET /api/scrape/status/{source_id}`

---

## ğŸ¯ Summary

**When you click "Scrape" button:**

1. **Frontend** sends HTTP POST to backend
2. **Backend** queues scraping task in background
3. **Backend** returns immediately ("Scraping started")
4. **Background thread** runs `_scrape_source_sync()`
5. **Scraper** (same as standalone script) does the work
6. **Results** saved to Supabase
7. **Status** visible in Sources page or logs

**It's the SAME scraping code**, just called through the API instead of directly!

