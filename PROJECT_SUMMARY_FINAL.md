# ğŸš€ Cargo News Aggregator - Complete Project Summary

## ğŸ“‹ What We've Built

A complete **Air Cargo News Aggregation System** that:

### âœ… Core Features
1. **Web Scraping** - Automatically scrapes articles from air cargo news websites
2. **AI Summarization** - Uses Google Gemini API to generate Traditional Chinese summaries
3. **Tag Extraction** - Automatically extracts tags (companies, topics, geography) from articles
4. **Database Storage** - Saves everything to Supabase (PostgreSQL)
5. **Web Interface** - Beautiful React/Next.js frontend to view and filter articles
6. **Duplicate Prevention** - Smart duplicate detection by URL and title
7. **Smart Pagination** - Scrapes more pages on first run, fewer on daily runs
8. **Background Processing** - Scraping runs in background without blocking the server

### ğŸ¯ Supported News Sources

1. **Air Cargo News** (`aircargonews.net`)
   - Custom scraper: `AircargonewsScraper`
   - Listing URL: `/latest-news/31.more?navcode=28`
   - Status: âœ… Working

2. **Air Cargo Week** (`aircargoweek.com`)
   - Custom scraper: `AircargoweekScraper`
   - Listing URL: `/news/`
   - Uses Playwright for dynamic content
   - Status: âœ… Working

### ğŸ—ï¸ Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Frontend      â”‚  Next.js/React (port 3000)
â”‚   (Web UI)      â”‚  - View articles
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Filter by tags
         â”‚           - Manage sources
         â”‚
         â”‚ HTTP API
         â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   Backend       â”‚  FastAPI (port 8000)
â”‚   (Python)      â”‚  - API endpoints
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜  - Background tasks
         â”‚           - Daily scheduler
         â”‚
         â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚          â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”  â”Œâ”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Scrapers   â”‚  â”‚  AI          â”‚
â”‚             â”‚  â”‚  Summarizer  â”‚
â”‚ - Base      â”‚  â”‚  (Gemini)    â”‚
â”‚ - Aircargo  â”‚  â”‚              â”‚
â”‚   News      â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
â”‚ - Aircargo  â”‚
â”‚   Week      â”‚
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”
â”‚  Supabase   â”‚  PostgreSQL Database
â”‚  (Database) â”‚  - Articles
â”‚             â”‚  - Sources
â”‚             â”‚  - Scraping logs
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### ğŸ“ Project Structure

```
Cargo News/
â”œâ”€â”€ app/                          # Backend (Python/FastAPI)
â”‚   â”œâ”€â”€ main.py                  # FastAPI app entry point
â”‚   â”œâ”€â”€ config.py                # Configuration (env vars)
â”‚   â”œâ”€â”€ api/
â”‚   â”‚   â””â”€â”€ routes/
â”‚   â”‚       â”œâ”€â”€ sources.py       # Source management API
â”‚   â”‚       â”œâ”€â”€ articles.py      # Article retrieval API
â”‚   â”‚       â””â”€â”€ scrape.py        # Scraping trigger API
â”‚   â”œâ”€â”€ scraper/
â”‚   â”‚   â”œâ”€â”€ base_scraper.py      # Base scraper class
â”‚   â”‚   â”œâ”€â”€ aircargonews_scraper.py
â”‚   â”‚   â”œâ”€â”€ aircargoweek_scraper.py
â”‚   â”‚   â””â”€â”€ scraper_factory.py   # Scraper selection
â”‚   â”œâ”€â”€ ai/
â”‚   â”‚   â””â”€â”€ summarizer.py        # Gemini AI integration
â”‚   â”œâ”€â”€ database/
â”‚   â”‚   â”œâ”€â”€ models.py            # Pydantic models
â”‚   â”‚   â””â”€â”€ supabase_client.py   # Database operations
â”‚   â””â”€â”€ scheduler/
â”‚       â””â”€â”€ daily_scraper.py     # Daily auto-scraping
â”‚
â”œâ”€â”€ frontend/                     # Frontend (Next.js/React)
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ page.tsx             # Articles page
â”‚   â”‚   â”œâ”€â”€ sources/
â”‚   â”‚   â”‚   â””â”€â”€ page.tsx         # Sources management
â”‚   â”‚   â””â”€â”€ articles/[id]/
â”‚   â”‚       â””â”€â”€ page.tsx         # Article detail
â”‚   â””â”€â”€ components/
â”‚       â”œâ”€â”€ ArticleList.tsx
â”‚       â”œâ”€â”€ SourceList.tsx
â”‚       â””â”€â”€ TagFilter.tsx
â”‚
â”œâ”€â”€ scrape_aircargoweek.py       # Standalone scraping script
â”œâ”€â”€ test_aircargoweek.py         # Testing script
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ database_schema.sql           # Database schema
â””â”€â”€ .env                          # Environment variables
```

### ğŸ”§ Key Technologies

- **Backend**: Python 3.x, FastAPI, Uvicorn
- **Frontend**: Next.js 14, React, TypeScript, Tailwind CSS
- **Database**: Supabase (PostgreSQL)
- **AI**: Google Gemini API
- **Scraping**: BeautifulSoup4, Playwright, Requests
- **Scheduling**: APScheduler
- **Deployment**: Railway (ready)

### ğŸš€ How to Run

#### Option 1: Full System (Recommended for Production)

**Terminal 1 - Backend:**
```bash
cd "/Users/sai/Cargo News"
source venv/bin/activate
uvicorn app.main:app --reload --port 8000
```

**Terminal 2 - Frontend:**
```bash
cd "/Users/sai/Cargo News/frontend"
npm run dev
```

Then visit:
- Frontend: http://localhost:3000
- Backend API: http://localhost:8000
- API Docs: http://localhost:8000/docs

#### Option 2: Standalone Script (For Testing)

**Run scraping script directly:**
```bash
cd "/Users/sai/Cargo News"
source venv/bin/activate
python3 scrape_aircargoweek.py --max-pages 3
```

### ğŸ“ Environment Variables (.env)

```bash
# Supabase
SUPABASE_URL=your_supabase_url
SUPABASE_KEY=your_supabase_key

# Google Gemini
GEMINI_API_KEY=AIzaSyCSg4pvORmJmdsfPLqmZ41Ia5v9kDNS1Dg

# Server
PORT=8000
SCRAPING_DELAY_SECONDS=2
MAX_RETRIES=3
```

### ğŸ¯ Main Workflows

#### 1. Adding a News Source
- Go to `/sources` page
- Click "Add Source"
- Enter URL (e.g., `https://aircargoweek.com/news/`)
- System automatically selects the right scraper
- Optionally auto-scrape immediately

#### 2. Scraping Articles
**Via Web Interface:**
- Click "Scrape" button next to a source
- Runs in background
- Articles appear as they're processed

**Via Script:**
- Run `python3 scrape_aircargoweek.py`
- See real-time progress
- Articles saved to database

#### 3. Viewing Articles
- Go to homepage (`/`)
- Filter by tags
- Click article to read full summary
- All summaries in Traditional Chinese

#### 4. Daily Automation
- Built-in scheduler runs at 00:00 UTC daily
- Scrapes all active sources automatically
- Smart pagination (stops early on duplicates)

### ğŸ”‘ Key Files to Run

#### For Full System:
1. **`app/main.py`** - Backend server (via `uvicorn app.main:app`)
2. **`frontend/app/page.tsx`** - Frontend (via `npm run dev`)

#### For Standalone Scraping:
1. **`scrape_aircargoweek.py`** - Complete scraping workflow

### ğŸ“Š Database Tables

1. **`news_sources`** - Registered news sources
2. **`articles`** - Scraped articles with summaries
3. **`scraping_logs`** - Scraping history and statistics

### âœ¨ Special Features

- **Smart Duplicate Detection**: Checks by URL and title similarity
- **Smart Pagination**: 
  - First run: Up to 100 pages
  - Daily runs: 3 pages, stops early on duplicates
- **Anti-Bot Measures**: Playwright fallback for blocked sites
- **Background Processing**: Non-blocking scraping via thread pool
- **Tag Extraction**: Automatic tagging (companies, topics, geography)
- **Traditional Chinese**: All summaries in Traditional Chinese

### ğŸ“ What You Can Do Now

1. âœ… Add news sources via web interface
2. âœ… Scrape articles manually (button or script)
3. âœ… View articles with Traditional Chinese summaries
4. âœ… Filter articles by tags
5. âœ… Automatic daily scraping (00:00 UTC)
6. âœ… Test scrapers before adding sources
7. âœ… View scraping logs and statistics

### ğŸš¨ Important Notes

- **Playwright**: Must be installed (`playwright install chromium`)
- **Environment**: Must have `.env` file with API keys
- **Database**: Must run `database_schema.sql` in Supabase
- **Backend**: Must be running for web scraping button to work
- **Frontend**: Connects to backend at `http://localhost:8000`

---

## ğŸ‰ Project Status: **COMPLETE & WORKING**

All features implemented and tested! ğŸš€

